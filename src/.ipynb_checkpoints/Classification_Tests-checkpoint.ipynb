{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spiegazione test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import xml.etree.ElementTree as ET\n",
    "import nltk\n",
    "from nltk.stem.snowball import ItalianStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CarWordsHandler:\n",
    "    # https://github.com/n8barr/automotive-model-year-data\n",
    "    def __init__(self, cars_file):\n",
    "        self.brands_list = set()\n",
    "        self.models_list = set()\n",
    "        f = open(cars_file, \"r\")\n",
    "        cars_list = f.read().splitlines()\n",
    "        for i in range(len(cars_list)):\n",
    "            brand = cars_list[i].split(',')[1][2:-1].lower()\n",
    "            model = cars_list[i].split(',')[2][2:-2].lower()\n",
    "            self.brands_list.add(brand)\n",
    "            self.models_list.add(model)\n",
    "        # remove some useless models\n",
    "        self.models_list.remove('i')\n",
    "        self.models_list.remove('gli')\n",
    "        self.models_list.remove('estate')\n",
    "        self.brands_list = list(self.brands_list)\n",
    "        self.models_list = list(self.models_list)\n",
    "        self.brands_list.sort()\n",
    "        self.models_list.sort()\n",
    "    \n",
    "    # binary search to get if a word is a brand \n",
    "    def isBrand(self, word):\n",
    "        word = word.lower()\n",
    "        first = 0\n",
    "        last = len(self.brands_list) -1\n",
    "        while first < last:\n",
    "            mid = int((last + first) / 2)\n",
    "            if word == self.brands_list[mid]:\n",
    "                return True\n",
    "            elif word < self.brands_list[mid]:\n",
    "                last = mid\n",
    "            elif word > self.brands_list[mid]:\n",
    "                first = mid\n",
    "            if last-first == 1:\n",
    "                if self.brands_list[first] == word or self.brands_list[last] == word:\n",
    "                    return True\n",
    "                else:\n",
    "                    return False\n",
    "    # binary search to get if a word is a brand \n",
    "    def isModel(self, word):\n",
    "        word = word.lower()\n",
    "        first = 0\n",
    "        last = len(self.models_list) -1\n",
    "        while first < last:\n",
    "            mid = int((last + first) / 2)\n",
    "            if word == self.models_list[mid]:\n",
    "                return True\n",
    "            elif word < self.models_list[mid]:\n",
    "                last = mid\n",
    "            elif word > self.models_list[mid]:\n",
    "                first = mid\n",
    "            if last-first == 1:\n",
    "                if self.models_list[first] == word or self.models_list[last] == word:\n",
    "                    return True\n",
    "                else:\n",
    "                    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding issues\n",
    "def correctEncodings(comment):\n",
    "    fin_comment = comment\n",
    "    fin_comment = re.sub('Ã¨', 'è', fin_comment)\n",
    "    fin_comment = re.sub('Ã©', 'é', fin_comment)\n",
    "    fin_comment = re.sub('Ã¬', 'ì', fin_comment)\n",
    "    fin_comment = re.sub('Ã²', 'ò', fin_comment)\n",
    "    fin_comment = re.sub('Ã¹', 'ù', fin_comment)\n",
    "    fin_comment = re.sub('Ã', 'à', fin_comment)\n",
    "    return fin_comment\n",
    "# recognize an URL\n",
    "def isURL(word):\n",
    "    # http://forum.rusconi.it/gentemotori/viewtopic.php ? t=434&sid=57c88f1b507d8f57717ea18e74e25324Â \n",
    "    return len(re.findall(\"^((http(s){0,1}://)|(www.))\\S+$\", word)) > 0\n",
    "# recognize an image tag\n",
    "def isPicture(word):\n",
    "    return len(re.findall(\"^<img.*>$\", word)) > 0\n",
    "# fix issues on urls\n",
    "def replaceURLs(comment):\n",
    "    return str(re.sub(r'(http(s){0,1}://|www.)(([^\\s]+)|/)+((\\s\\?\\s)[^\\s]+){0,1}', 'URL', comment)).replace(u'\\xa0', u' ')\n",
    "# replace images\n",
    "def replaceIMGs(comment):\n",
    "    return str(re.sub(r'<img.+>', 'IMG', comment))\n",
    "# replace brands\n",
    "def replaceBrands(cwhandler, comment):\n",
    "    tokens = comment.split(' ')\n",
    "    for i in range(len(tokens)):\n",
    "        if cwhandler.isBrand(tokens[i]):\n",
    "            tokens[i] = 'BRAND'\n",
    "    return ' '.join(tokens)\n",
    "# replace models\n",
    "def replaceModels(cwhandler, comment):\n",
    "    tokens = comment.split(' ')\n",
    "    for i in range(len(tokens)):\n",
    "        if cwhandler.isModel(tokens[i]):\n",
    "            tokens[i] = 'MODEL'\n",
    "    return ' '.join(tokens)\n",
    "# replace question marks\n",
    "def replaceQMarks(comment):\n",
    "    comment = re.sub(r'\\?{2,}', ' MULTI_QMARK', comment)\n",
    "    comment = re.sub(r'\\?', ' QMARK', comment)\n",
    "    return comment\n",
    "# replace esclamation marks\n",
    "def replaceEMarks(comment):\n",
    "    comment = re.sub(r'\\!{2,}', ' MULTI_EMARK', comment)\n",
    "    comment = re.sub(r'\\!', ' EMARK', comment)\n",
    "    return comment\n",
    "# remove character repetitions\n",
    "def removeRepeat(comment):\n",
    "    return re.sub(r'(a-zA-Z)\\1{2,}', r'\\1\\1\\1', comment)\n",
    "# replace speed\n",
    "def replaceSpeed(comment):\n",
    "    return re.sub(r'([0-9\\.*]+(\\s*(\\-|\\/|\\s)\\s*)+){0,1}[0-9\\.*]+(\\s*)(km/h|mph)', 'SPEED', comment)\n",
    "# replace consumption\n",
    "def replaceConsumption(comment):\n",
    "    return re.sub(r'([0-9\\.*]+(\\s*(\\-|\\/|\\s)\\s*)+){0,1}[0-9\\.*]+(\\s*)(km/l|mpg)', 'CONSUMPTION', comment)\n",
    "# replace weight\n",
    "def replaceWeight(comment):\n",
    "    return re.sub(r'[0-9\\.*]+(\\s*)(kg|tonnellate|ton|chili|kili)', 'WEIGHT', comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ItalianWordsHandler:\n",
    "    # https://dspace-clarin-it.ilc.cnr.it/repository/xmlui/handle/20.500.11752/ILC-73\n",
    "    def __init__(self, words_file):\n",
    "        # words information\n",
    "        self.words_dict = dict()\n",
    "        root = ET.parse(words_file).getroot()\n",
    "        for entry in root.findall('Lexicon/LexicalEntry'):\n",
    "            word = entry.find('Lemma').get('writtenForm')\n",
    "            pos = entry.get('partOfSpeech')\n",
    "            senti = entry.find('Sense/Sentiment').get('polarity')\n",
    "            conf = entry.find('Sense/Confidence').get('score')\n",
    "            self.words_dict[word] = {'POS': pos, 'Sentiment': senti, 'Confidence': conf}\n",
    "        # stemmer\n",
    "        self.it_stem = ItalianStemmer()\n",
    "        \n",
    "    # get word info. None if not exists\n",
    "    def getWordInfo(self, word):\n",
    "        # fields: POS, Sentiment, Confidence\n",
    "        return self.words_dict.get(word)\n",
    "    \n",
    "    # italian stemmer http://snowball.tartarus.org/algorithms/italian/stemmer.html\n",
    "    def stem(self, word):\n",
    "        return self.it_stem.stem(word)\n",
    "    \n",
    "    # correct words\n",
    "    def correctWords(self, text):\n",
    "        # not yet implemented\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessor:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.cwh = CarWordsHandler('resources/cars_data.sql')\n",
    "        self.iwh = ItalianWordsHandler('resources/ita_opeNER.xml')\n",
    "    # preprocess text\n",
    "    # allowed methods: word, swnt, pos\n",
    "    # ner (named entity recognition), replacing for instance 100 km/h with SPEED\n",
    "    def preprocessText(self, text, method='word', use_stemmer=False, ner=False):\n",
    "        if method not in ['word', 'pos', 'swnt']:\n",
    "            raise ValueError('Method not recognized. Select from word, swnt, pos')\n",
    "        # correct encodings (not yet implemented)\n",
    "        fin_text = correctEncodings(text)\n",
    "        # some basic preprocessing\n",
    "        fin_text = fin_text.lower()\n",
    "        # correct words (not yet)\n",
    "        fin_text = self.iwh.correctWords(fin_text)\n",
    "        # manage repetitions\n",
    "        fin_text = removeRepeat(fin_text)\n",
    "        # manage punctation\n",
    "        fin_text = replaceQMarks(fin_text)\n",
    "        fin_text = replaceEMarks(fin_text)\n",
    "        # manage URLs\n",
    "        fin_text = replaceURLs(fin_text)\n",
    "        # manage Images\n",
    "        fin_text = replaceIMGs(fin_text)\n",
    "        # NOW DEPENDS ON NER\n",
    "        if ner:\n",
    "            # manage brands and models\n",
    "            fin_text = replaceBrands(self.cwh, fin_text)\n",
    "            fin_text = replaceModels(self.cwh, fin_text)\n",
    "            # manage speed consumption and weight\n",
    "            fin_text = replaceSpeed(fin_text)\n",
    "            fin_text = replaceConsumption(fin_text)\n",
    "            fin_text = replaceWeight(fin_text)\n",
    "        # NOW DEPENDS ON METHOD\n",
    "        if method == 'word':\n",
    "            # just do nothing except eventually stemming\n",
    "            if use_stemmer:\n",
    "                tokens = fin_text.split(' ')\n",
    "                fin_text = ' '.join([t if t.isupper() else self.iwh.stem(t) for t in tokens]) \n",
    "        elif method == 'swnt':\n",
    "            tokens = fin_text.split(' ')\n",
    "            swnt_tokens = []\n",
    "            for t in tokens:\n",
    "                info = self.iwh.getWordInfo(t)\n",
    "                if info == None or info['Sentiment'] == None:\n",
    "                    swnt_tokens.append(t)\n",
    "                else:\n",
    "                    # confidence 0-100\n",
    "                    swnt_tokens.append(str(info['Sentiment'])[:3].upper() + '_' + str(int(float(info['Confidence'])*10)))\n",
    "            fin_text = ' '.join(swnt_tokens)\n",
    "            # stemmer\n",
    "            if use_stemmer:\n",
    "                tokens = fin_text.split(' ')\n",
    "                fin_text = ' '.join([t if t.isupper() else self.iwh.stem(t) for t in tokens]) \n",
    "        elif method == 'pos':\n",
    "            tokens = fin_text.split(' ')\n",
    "            pos_tokens = []\n",
    "            # pos\n",
    "            for t in tokens:\n",
    "                info = self.iwh.getWordInfo(t)\n",
    "                if info == None or info['POS'] == None:\n",
    "                    # unknown tag\n",
    "                    pos_tokens.append('UNK')\n",
    "                else:\n",
    "                    pos_tokens.append(str(info['POS']).upper())\n",
    "            # pos_word\n",
    "            for t in tokens:\n",
    "                info = self.iwh.getWordInfo(t)\n",
    "                if info == None or info['POS'] == None:\n",
    "                    pos_tokens.append('UNK_' + str(t))\n",
    "                else:\n",
    "                    pos_tokens.append(str(info['POS']).upper() + '_' + str(t))\n",
    "                    \n",
    "            fin_text = ' '.join(pos_tokens)\n",
    "            # stemmer\n",
    "            if use_stemmer:\n",
    "                tokens = fin_text.split(' ')\n",
    "                fin_text = ' '.join([t if t.isupper() else self.iwh.stem(t) for t in tokens]) \n",
    "        \n",
    "        return fin_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vectorizer:\n",
    "    \n",
    "    def __init__(self, list_comments, method='bow', max_features=1000, ngrams=2, just_presence=False):\n",
    "        if method not in ['bow', 'tfidf']:\n",
    "            raise ValueError('Method not recognized. Select from bow, tfidf')\n",
    "        if method == 'bow':\n",
    "            self.vectorizer = CountVectorizer(ngram_range=(1,ngrams), binary=just_presence, lowercase=False, max_features=max_features)\n",
    "        elif method == 'tfidf':\n",
    "            self.vectorizer = TfidfVectorizer(ngram_range=(1,ngrams), lowercase=False, max_features=max_features)\n",
    "        # fit vectorizer\n",
    "        self.vectorizer.fit(list_comments)          \n",
    "        \n",
    "        \n",
    "        '''\n",
    "        self.list_comments = list_comments\n",
    "        # initialize tfidf weights\n",
    "        self.idf_dict = {}\n",
    "        idf_dict_uni = {}\n",
    "        idf_dict_big = {}\n",
    "        # unigrams\n",
    "        for comment in list_comments:\n",
    "            tokens = list(set(comment.split()))\n",
    "            for t in tokens:\n",
    "                if idf_dict_uni.get(t) != None:\n",
    "                    idf_dict_uni[t] = idf_dict_uni[t] + 1\n",
    "                else:\n",
    "                    idf_dict_uni[t] = 1\n",
    "        # bigrams\n",
    "        for comment in list_comments:\n",
    "            tokens = comment.split()\n",
    "            for i in range(len(tokens) -2):\n",
    "                big = (tokens[i], tokens[i+1])\n",
    "                if idf_dict_big.get(big) != None:\n",
    "                    idf_dict_big[big] = idf_dict_big[big] + 1\n",
    "                else:\n",
    "                    idf_dict_big[big] = 1\n",
    "        # cut most frequent\n",
    "        idf_dict_uni = Counter(idf_dict_uni).most_common(most_common_unigrams)\n",
    "        idf_dict_big = Counter(idf_dict_big).most_common(most_common_bigrams)\n",
    "        self.idf_dict.update(idf_dict_uni)\n",
    "        self.idf_dict.update(idf_dict_big)\n",
    "        '''\n",
    "        \n",
    "    def vectorize(self, comment):\n",
    "        \n",
    "        return self.vectorizer.transform([comment])\n",
    "        \n",
    "        '''\n",
    "        if method not in ['bow', 'tfidf']:\n",
    "            raise ValueError('Method not recognized. Select from bow, tfidf')\n",
    "        unigrams = comment.split(' ')\n",
    "        bigrams = []\n",
    "        for i in range(len(unigrams) -2):\n",
    "            bigrams.append((unigrams[i], unigrams[i+1]))\n",
    "        if method == 'bow':\n",
    "            bow_dict = dict.fromkeys(self.idf_dict, 0)\n",
    "            for u in unigrams + bigrams:\n",
    "                if bow_dict.get(u) != None:\n",
    "                    if just_presence:\n",
    "                        bow_dict[u] = 1\n",
    "                    else:\n",
    "                        bow_dict[u] = bow_dict[u] +1\n",
    "            return list(bow_dict.values())\n",
    "        elif method == 'tfidf':\n",
    "            tf_dict = dict.fromkeys(self.idf_dict, 0)\n",
    "            for u in unigrams + bigrams:\n",
    "                if tf_dict.get(u) != None:\n",
    "                    tf_dict[u] = tf_dict[u] +1\n",
    "        '''\n",
    "        \n",
    "    def get_feature_names(self):\n",
    "        return self.vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ciao', 'ciao come', 'ciao questa', 'come', 'come scrive', 'come va', 'falso', 'lo', 'lo so', 'me', 'me falso', 'non', 'non lo', 'per', 'per me', 'questa', 'questa tastiera', 'scrive', 'scrive ciao', 'so', 'so per', 'tastiera', 'va', 'vediamo', 'vediamo come']\n",
      "[[0.62663214 0.41197298 0.         0.31331607 0.         0.41197298\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.41197298 0.\n",
      "  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "v = Vectorizer(list_comments=['ciao come va ?', 'vediamo come scrive ciao questa tastiera', 'non lo so, per me è falso'], method='tfidf', max_features=100, ngrams=2, just_presence=True)\n",
    "print(v.get_feature_names())\n",
    "print(v.vectorize('ciao ciao come va ?').toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sono reali calcolati nel arco del tutto anno nel estate qualcosa in piÃ¹ causa gomme di 17\" e climatizzatore nel inverno un po di meno. Per quanto riguarda le autostrade quelle che percorro io principalmente la A4 e molto congestionata cosi spesso la media e 110-115 km/h che ovviamente influisce positivamente a i consumi. Ma quello che mi piace di piÃ¹ Ã¨ assenza dei guasti. Sulla vecchia Accord il primo guasto lo ho avuto a 200000 km si Ã¨ rotto il termostato della clima. Ogni tanto faccio giro di altri forum e leggo delle turbine rotte catene di distribuzione progettate male iniettori fatti male mah nel 2015 per me sono le cose incomprensibili . Con tutti gli difetti che puÃ² avere preferisco la Honda. \n",
      "##########################################################################################\n",
      "sono reali calcolati nel arco del tutto anno nel estate qualcosa in più causa gomme di 17\" e climatizzatore nel inverno un po di meno. per quanto riguarda le autostrade quelle che percorro io principalmente la a4 e molto congestionata cosi spesso la media e 110-115 km/h che ovviamente influisce positivamente a i consumi. ma quello che mi piace di più è assenza dei guasti. sulla vecchia accord il primo guasto lo ho avuto a 200000 km si è rotto il termostato della clima. ogni tanto faccio giro di altri forum e leggo delle turbine rotte catene di distribuzione progettate male iniettori fatti male mah nel 2015 per me sono le cose incomprensibili . con tutti gli difetti che può avere preferisco la honda. \n",
      "##########################################################################################\n",
      "sono POS_2 calcolati nel NEU_2 del NEU_2 anno nel estate qualcosa in più POS_5 gomme di 17\" e climatizzatore nel inverno un po di meno. per quanto riguarda le autostrade quelle che percorro POS_1 principalmente la a4 e molto congestionata cosi spesso la NEU_2 e 110-115 km/h che ovviamente influisce POS_10 a i consumi. ma quello che mi piace di più è NEG_6 dei guasti. sulla vecchia accord il POS_6 NEG_5 lo ho avuto a 200000 km si è NEG_5 il termostato della clima. ogni NEG_5 faccio NEU_2 di altri forum e leggo delle turbine rotte catene di NEU_5 progettate NEG_6 iniettori fatti NEG_6 mah nel 2015 per me sono le cose incomprensibili . con tutti gli difetti che può NEG_5 preferisco la honda. \n",
      "##########################################################################################\n",
      "UNK NOUN UNK UNK NOUN UNK NOUN UNK UNK UNK UNK UNK ADV NOUN UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK NOUN UNK UNK UNK UNK ADV UNK UNK UNK UNK NOUN UNK UNK UNK UNK UNK UNK ADV UNK UNK UNK UNK UNK UNK UNK UNK UNK ADV UNK NOUN UNK UNK UNK UNK UNK UNK ADJ NOUN UNK UNK UNK UNK UNK UNK UNK UNK ADJ UNK UNK UNK UNK UNK ADJ UNK NOUN UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK NOUN UNK NOUN UNK UNK NOUN UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK VERB UNK UNK UNK UNK UNK_sono NOUN_reali UNK_calcolati UNK_nel NOUN_arco UNK_del NOUN_tutto UNK_anno UNK_nel UNK_estate UNK_qualcosa UNK_in ADV_più NOUN_causa UNK_gomme UNK_di UNK_17\" UNK_e UNK_climatizzatore UNK_nel UNK_inverno UNK_un UNK_po UNK_di UNK_meno. UNK_per UNK_quanto UNK_riguarda UNK_le UNK_autostrade UNK_quelle UNK_che UNK_percorro NOUN_io UNK_principalmente UNK_la UNK_a4 UNK_e ADV_molto UNK_congestionata UNK_cosi UNK_spesso UNK_la NOUN_media UNK_e UNK_110-115 UNK_km/h UNK_che UNK_ovviamente UNK_influisce ADV_positivamente UNK_a UNK_i UNK_consumi. UNK_ma UNK_quello UNK_che UNK_mi UNK_piace UNK_di ADV_più UNK_è NOUN_assenza UNK_dei UNK_guasti. UNK_sulla UNK_vecchia UNK_accord UNK_il ADJ_primo NOUN_guasto UNK_lo UNK_ho UNK_avuto UNK_a UNK_200000 UNK_km UNK_si UNK_è ADJ_rotto UNK_il UNK_termostato UNK_della UNK_clima. UNK_ogni ADJ_tanto UNK_faccio NOUN_giro UNK_di UNK_altri UNK_forum UNK_e UNK_leggo UNK_delle UNK_turbine UNK_rotte UNK_catene UNK_di NOUN_distribuzione UNK_progettate NOUN_male UNK_iniettori UNK_fatti NOUN_male UNK_mah UNK_nel UNK_2015 UNK_per UNK_me UNK_sono UNK_le UNK_cose UNK_incomprensibili UNK_. UNK_con UNK_tutti UNK_gli UNK_difetti UNK_che UNK_può VERB_avere UNK_preferisco UNK_la UNK_honda. UNK_\n"
     ]
    }
   ],
   "source": [
    "text = 'Sono reali calcolati nel arco del tutto anno nel estate qualcosa in piÃ¹ causa gomme di 17\" e climatizzatore nel inverno un po di meno. Per quanto riguarda le autostrade quelle che percorro io principalmente la A4 e molto congestionata cosi spesso la media e 110-115 km/h che ovviamente influisce positivamente a i consumi. Ma quello che mi piace di piÃ¹ Ã¨ assenza dei guasti. Sulla vecchia Accord il primo guasto lo ho avuto a 200000 km si Ã¨ rotto il termostato della clima. Ogni tanto faccio giro di altri forum e leggo delle turbine rotte catene di distribuzione progettate male iniettori fatti male mah nel 2015 per me sono le cose incomprensibili . Con tutti gli difetti che puÃ² avere preferisco la Honda. '\n",
    "print(text)\n",
    "print('##########################################################################################')\n",
    "p = Preprocessor()\n",
    "print(p.preprocessText(text, ner=False, use_stemmer=False, method='word'))\n",
    "print('##########################################################################################')\n",
    "print(p.preprocessText(text, ner=False, use_stemmer=False, method='swnt'))\n",
    "print('##########################################################################################')\n",
    "print(p.preprocessText(text, ner=False, use_stemmer=False, method='pos'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
