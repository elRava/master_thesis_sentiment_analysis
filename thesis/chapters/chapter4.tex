%!TEX root = ../dissertation.tex
%\begin{savequote}[75mm]
%Nulla facilisi. In vel sem. Morbi id urna in diam dignissim feugiat. Proin molestie tortor eu velit. Aliquam erat volutpat. Nullam ultrices, diam tempus vulputate egestas, eros pede varius leo.
%\qauthor{Quoteauthor Lastname}
%\end{savequote}

\chapter{Experiments}

Summarizing the work done until now, it has been created a sentiment analysis dataset based on italian automotive forums by crawling web resources and then scraping HTML pages. Then, the dataset has been labeled by many workers, obtaining a sufficiently large amount of data to be processed. The next phase consisted in defining some machine learning models in order to make both topic detection and sentiment classification. In this Chapter, will be firstly presented, as a benchmark, results obtained running the models on a Twitter's dataset, and then the results obtained on the created dataset.\\
Every test will be presented with a common framework: 
\begin{itemize}
	\item Results obtained with a classification before features selection;
	\item Features selection and most important features;
	\item Results obtained after features selection;
\end{itemize}
Classification results will be presented both visually using confusion matrices and using numerical scores.\\
Since used algorithms don't require huge amount of computational power, runs have been made on a 2019 consumer technology with the following specifics:
% CPU, RAM,
\begin{center}
	\begin{tabular}{ |c||c| } 
		\hline
		\textbf{CPU} & Intel(R) Core(TM) i7-8550U CPU @ 1.80GHz 4 cores, 8 threads\\ 
		\hline
		\textbf{RAM} & 16GB DDR4 \\
		\hline
		\textbf{O.S.} & Ubuntu 18.04 \\
		\hline
	\end{tabular}
\end{center}
Where it has been installed a Python 3.7 release on a Jupyter Notebook.


\section{Experiments on Twitter's Airline Dataset}

The first series of experiments concerns the Twitter's Airline dataset described in Section 2.1.6. The purpose of these tests is to verify the goodness of the models in a dataset that is plenty of well labeled comments. The reference result on tweets sentiment classification can be extracted from \cite{Zimbra:2018:STS:3210372.3185045}, where BPEF algorithm reached the best results with an average accuracy of 71.38\% on sentiment classification on five different datasets, but average performance of all models stay around 65\%, so a similar result will be positive.\\
The dataset consists in 14640 labeled tweets, divided into 3099 positives, 2363 neutrals and 9178 negatives. Due to the plenty of data, all classes were balanced, with the number of comments of the minority class. Successively, the dataset were divided into training set and test set, respectively the 80\% and the 20\%. Moreover, the training set were further divided into actual training set and validation set, again respectively the 80\% and the 20\%. The distribution of the datasets is:

\begin{center}
	\begin{tabular}{ | c  c  c  c | c | } 
		\hline
		& \textbf{Positives} & \textbf{Nautrals} & \textbf{Negatives} & \textbf{Total} \\
		\hline
		\textbf{Training} & 960 & 960 & 960 & 2880 \\ 
		\hline
		\textbf{Validation} & 240 & 240 & 240 & 720 \\ 
		\hline
		% TODO riempi test e fai classificazione
		\textcolor{red}{TEST}
		\textbf{Test} &  &  & &\\
		\hline
		%\label{table:twitt-class-data}
		%\caption{Class distribution of training, validation and test sets of tweets' sentiment classification.}
	\end{tabular}
\end{center}

Due to the class balance, also accuracy score is meaningful, so it is presented along with F1-macro and F1-micro scores.


\subsection{Sentiment Classification with SVM}

After preprocessing and vectorization with TF-IDF method on the training set, involving both unigrams and bigrams, the outcome dimension counts 22.486 features. Data are stored in a sparse matrix 2880x22.486 with 50.006 stored elements, so as expected, it is actually very sparse.\\
A first model train is performed involving all features, optimizing the regularization parameter $C$ from 7 candidates exponentially equally spaced from $10^{-3}$ to $10^3$, searching for the best F1-macro score.\\
The selected model is the one with $C$=1, and the classification results obtained with the validation set are:

% SVM no fs
% TODO sistema conf matrix
\begin{figure}[ht]
	\centering
	\includegraphics[width=0.3\textwidth]{figures/conf_matrices/twitter_sent_svm/tw_snt_svm_bfs.pdf}
	\label{fig:conf}
\end{figure}

\begin{center}
	\begin{tabular}{ | c | c | } 
		\hline
		\textbf{F1-macro} & 0.734 \\
		\hline
		\textbf{F1-micro} & 0.733 \\ 
		\hline
		\textbf{Accuracy} & 0.734 \\ 
		\hline
	\end{tabular}
\end{center}

It is possible to see that the baseline method without feature selection reaches good results, aligned with state of the art. From the confusion matrix, diagonal values (which are the correctly classified) are in fact the majority, and also the accuracy of 70,4\% reflects the visual considerations.\\
The sorted weights of the three binary classifiers that constitute the one-versus-one multiclass classifier are shown in Figure \ref{fig:svm-fs}.

% fs

\begin{figure}[H]
	\centering
	\begin{subfigure}{1\textwidth} % width of left subfigure
		\includegraphics[width=\textwidth]{figures/conf_matrices/twitter_sent_svm/svm_fs_1.png}
	\end{subfigure}
	\begin{subfigure}{1\textwidth} % width of right subfigure
		\includegraphics[width=\textwidth]{figures/conf_matrices/twitter_sent_svm/svm_fs_2.png}
	\end{subfigure}
	\begin{subfigure}{1\textwidth} % width of right subfigure
	\includegraphics[width=\textwidth]{figures/conf_matrices/twitter_sent_svm/svm_fs_3.png}
	\end{subfigure}
	\caption{Features' weights of the three binary classifiers of the one-versus-one multiclass classifier} % caption for whole figure
	\label{fig:svm-fs}
\end{figure}

After some manual tries, the selected cutoff values are respectively 0.3, 0.3 and 0.2, obtaining 9564 final selected features, where most important with respect every binary classifier are shown in Figure % TODO features piu importanti svm
\textcolor{red}{FEATURE PIU IMPORTANTI}
% SVM fs
 Retraining the model using just selected features, the results on the validation set are:
% TODO sistemare
\begin{figure}[H]
	\centering
	\includegraphics[width=0.3\textwidth]{figures/conf_matrices/twitter_sent_svm/tw_snt_svm_afs.pdf}
	\label{fig:conf1}
\end{figure}

\begin{center}
	\begin{tabular}{ | c | c | } 
		\hline
		\textbf{F1-macro} & 0.702 \\
		\hline
		\textbf{F1-micro} & 0.704 \\ 
		\hline
		\textbf{Accuracy} & 0.704 \\ 
		\hline
	\end{tabular}
\end{center}

After feature selection it is possible to notice a loss of performance due to the removal of some relevant features. However, features selection has the goal to make the model more stable, rather that more performing. The final metrics still highlight performance close to state of the art models, and looking at the result on test data, .... % TODO test classfication

\textcolor{red}{TEST CLASSIFICATION}


\subsection{Sentiment Classification with Revised BPEF}

At this point the goal is to enhance the previous model with the revised BPEF ensemble. In this model, feature's relevance is calculated using the information gain metric, and since it is based on dataset's distribution instead of model's weights, it is possible to calculate it as the first phase, for every combination of the model's features parameters.\\
The outcome of features ranking is very similar for all features parameters and has the following trend:

\begin{figure}[ht]
	\centering
	\includegraphics[width=\textwidth]{figures/conf_matrices/twitter_sent_bpef/bpef_fs_1.png}
	\label{fig:conf2}
\end{figure}

It is possible to notice that there are just few features with higher information gain, while most have a flat trend. The strategy for features selection was to set the cutoff value in correspondence of the starting of the flat curve, in order to keep just features with higher information gain.\\
Before features selection it were counted the following number of features:

\begin{center}
	\begin{tabular}{ c  c  c } 
		\hline
		\textbf{Feature name} & \textbf{Word summarizing} & \textbf{\# Features} \\
		\hline
		word & false & 24.206 \\ 
		\hline
		word & true & 22.485 \\ 
		\hline
		pos & false & 30.282 \\ 
		\hline
		pos & true & 28.318 \\ 
		\hline
		swnt & false & 22.789 \\ 
		\hline
		swnt & true & 21.113 \\ 
		\hline
	\end{tabular}
\end{center}

Recalling that the involved algorithms are SVM, Logistic Regression, Ra{\"i}ve Bayes and Random Forest, they are all trained optimizing with a grid search on the followings parameters:
\begin{itemize}
	\item SVM: $C$ from $10^{-3}$ to $10^3$ with 7 exponentially evenly spaced values;
	\item Logistic Regression: $C$ from $10^{-3}$ to $10^3$ with 7 exponentially evenly spaced values;
	\item Random Forest: 
	\begin{itemize}
		\item Number of estimators: [201, 501]
		\item Maximum of looked features on split: [auto, $log_2$]
		\item Maximum depth of the tree: [10, 100]
		\item Split criterion: [gini, entropy]
	\end{itemize}
\end{itemize}

% BPEF no fs
The classification without features selection on validation data gives the following results:

% TODO bpef twitter nofs conf matrix
\textcolor{red}{CONF MATRIX BPEF NOFS}
\begin{center}
	\begin{tabular}{ | c | c | } 
		\hline
		\textbf{F1-macro} & 0.732 \\
		\hline
		\textbf{F1-micro} & 0.731 \\ 
		\hline
		\textbf{Accuracy} & 0.731 \\ 
		\hline
	\end{tabular}
\end{center}

% fs

The results are very similar to the ones reached with SVM classificator without features selection. Setting a cutoff value for feature selection equal to $4\times10^{-4}$, the number of selected features becomes:

\begin{center}
	\begin{tabular}{ c  c  c } 
		\hline
		\textbf{Feature name} & \textbf{Word summarizing} & \textbf{\# Features} \\
		\hline
		word & false & 2.665 \\ 
		\hline
		word & true & 2.622 \\ 
		\hline
		pos & false & 3.181 \\ 
		\hline
		pos & true & 3.057 \\ 
		\hline
		swnt & false & 2.657 \\ 
		\hline
		swnt & true & 2.612 \\ 
		\hline
	\end{tabular}
\end{center}

% BPEF fs

Which are less than one third than the previously case. Training again the model optimizing the same hyperparameters with the same grid search, the obtained results are the followings:

% TODO matrice confusione bpef twitter fs
\textcolor{red}{BOEF CONF MATRIX FS}

\begin{center}
	\begin{tabular}{ | c | c | } 
		\hline
		\textbf{F1-macro} & 0.753 \\
		\hline
		\textbf{F1-micro} & 0.751 \\ 
		\hline
		\textbf{Accuracy} & 0.751 \\ 
		\hline
	\end{tabular}
\end{center}

Looking at the scores, it is easy to notice the better performance of the BPEF revisitation against the SVM model, even considering very less features. Moreover, due to the strong cut of the features and the optimal results, it is possible to suppose that the feature selection method returns an actual set of relevant features, that combined with the stability of the ensemble lead to prefer this model against the previous one. These considerations are confirmed looking at the results on the test set:

% TODO risultati test set
\textcolor{red}{RISULTATI TEST SET}


\section{Experiments on Italian's Automotive Dataset}

Now that some results are available, it is possible to state the effectiveness of the implemented algorithms. In this section the same models for sentiment analysis will be executed on the Italian automotive dataset. Moreover, have been designed models for relevance detection that will be implemented for the cascade classifier.\\

\subsubsection{Relevance Detection}

Before sentiment classification, it has been studied the task of relevance detection, where the goal consists on identifying weather a comment talks about a given topic. In the dataset, relevance is identified in the presence of a sentiment expression, whether it is "positive", "negative" or "neutral", while the irrelevance is obviously identified in the "irrelevant" label. The problem is then summarized in a binary text classification problem, that will constitute the first block of the cascade classificator.\\
Relevance is studied one topic at time, in fact the whole study of the models was made on the class "Engine", and then exported for all others. For the class "Engine" the dataset is constituted of 7183 comments, 866 of which are classified as "relevant", so as said, unbalance is really visible. The dataset is splitted into training data and test data, respectively the 80\% and 20\%. Then, the training dataset has been further splitted into actual training and validation sets, respectively the 80\% and 20\%. The distribution of the datasets is:

\begin{center}
	\begin{tabular}{ | c  c  c | c | } 
		\hline
		& \textbf{Relevant} & \textbf{Irrelevant} & \textbf{Total} \\
		\hline
		% TODO riempi training
		
		\textbf{Training} \textcolor{red}{TRAINING} &  &  &  \\ 
		\hline
		\textbf{Validation} & 133 & 1017 & 1150 \\ 
		\hline
		% TODO riempi test e fai classificazione
		\textbf{Test} \textcolor{red}{TEST} &  &  & \\
		\hline
		%\label{table:twitt-class-data}
		%\caption{Class distribution of training, validation and test sets of tweets' sentiment classification.}
	\end{tabular}
\end{center}



\subsubsection{Sentiment Classification}







\subsection{Relevance Detection with SVM}

% SVM no fs

% fs

% SVM fs

\subsection{Relevance Detection with Logistic Regression}

% LR no fs

% fs

% LR fs

\subsection{Sentiment Classification with SVM}

% SVM no fs

% fs

% SVM fs

\subsection{Sentiment Classification with Revised BPEF}

% BPEF no fs

% fs

% BPEF fs

\subsection{4-labels Classification with SVM}

% cascade SVM

\subsection{4-labels Classification with Cascade Classifier}

% cascade BPEF

\subsection{4-labels Classification with Test Data}

% SVM

% BPEF

\subsection{Data Visualization}